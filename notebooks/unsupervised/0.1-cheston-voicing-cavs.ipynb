{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from deep_pianist_identification import utils\n",
    "from deep_pianist_identification.extractors import get_piano_roll\n",
    "from pretty_midi import PrettyMIDI\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import groupby\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "SPLIT_PATH = os.path.join(utils.get_project_root(), \"references/_unsupervised_resources/voicings/midi_split/\")\n",
    "class VoicingLoaderReal(Dataset):\n",
    "    def __init__(self, cav_number, n_clips: int = None):\n",
    "        super().__init__()\n",
    "        self.cav_midis = self.get_midi_paths(cav_number, n_clips)\n",
    "        midis = [list(self.create_midi(midi)) for midi in self.cav_midis]\n",
    "        self.midis = [m for ms in midis for m in ms]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_midi_paths(cav_number, n_clips):\n",
    "        cav_path = os.path.join(SPLIT_PATH, f\"{cav_number}_cav\")\n",
    "        assert os.path.isdir(cav_path)\n",
    "        cavs = [os.path.join(cav_path, i) for i in os.listdir(cav_path) if i.endswith('mid')]\n",
    "        if n_clips is not None:\n",
    "            shuffle(cavs)\n",
    "            cavs = cavs[:n_clips]\n",
    "        return cavs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.midis)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[np.ndarray, int]:\n",
    "        # Return the MIDI piano roll and the class index (i.e., True)\n",
    "        return self.midis[idx], 1\n",
    "\n",
    "    def create_midi(self, cav_midi_path):\n",
    "        pm = PrettyMIDI(cav_midi_path)\n",
    "        lhand = [(i.start, i.end, i.pitch, i.velocity) for i in pm.instruments[1].notes]\n",
    "        rhand = [(i.start, i.end, i.pitch, i.velocity) for i in pm.instruments[0].notes]\n",
    "        # Take top note from each left hand if we have multiple voices\n",
    "        newlhand = []\n",
    "        for key, group in groupby(lhand, lambda x: x[0]):\n",
    "            group = list(group)\n",
    "            if len(group) > 1:\n",
    "                # Get the highest pitch\n",
    "                sort = sorted(group, key=lambda x: x[2])\n",
    "                rhand.extend(sort[1:])\n",
    "                newlhand.append(sort[0])\n",
    "        lhand = newlhand\n",
    "        \n",
    "        rhand = sorted(rhand, key=lambda x: (x[0], x[2]))\n",
    "        rhand_grp = [list(i[1]) for i in groupby(rhand, lambda x: x[0])]\n",
    "        \n",
    "        \n",
    "        inverted_rhands = {i: [] for i in range(len(rhand_grp[0]) - 1)}\n",
    "        for inversion in inverted_rhands.keys():\n",
    "            for grp in rhand_grp:\n",
    "                to_invert = sorted(grp, key=lambda x: x[2])[:inversion + 1]\n",
    "                inverted = [(i[0], i[1], i[2] + 12, i[3]) for i in to_invert]\n",
    "                combined = inverted + grp[inversion + 1:]\n",
    "                inverted_rhands[inversion].extend(sorted(combined, key=lambda x: x[2]))\n",
    "                      \n",
    "        to_compute = [rhand, * list(inverted_rhands.values())]\n",
    "        # Iterating over all inversions\n",
    "        for rhands in to_compute:\n",
    "            for transp in range(-11, 12):\n",
    "                for include_lh in [True, False]:\n",
    "                    if include_lh:\n",
    "                        comb = sorted(rhands + lhand, key=lambda x: x[0])\n",
    "                    else:\n",
    "                        comb = sorted(rhands, key=lambda x: x[0])\n",
    "                    onset_grps = [list(i[1]) for i in groupby(comb, lambda x: x[0])]\n",
    "                    spacer = np.linspace(0, utils.CLIP_LENGTH, len(onset_grps))\n",
    "                    fmt = []\n",
    "                    for start, end, grp in zip(spacer, spacer[1:], onset_grps):\n",
    "                        fmt_grp = [(start, end, note[2] + transp, note[3]) for note in grp]\n",
    "                        fmt.extend(fmt_grp)\n",
    "                    yield get_piano_roll(fmt)\n",
    "\n",
    "\n",
    "class VoicingLoaderFake(VoicingLoaderReal):\n",
    "    def __init__(self, avoid_cav_number, n_clips):\n",
    "        super().__init__(cav_number=avoid_cav_number, n_clips=n_clips)\n",
    "        self.cav_midis = self.cav_midis[:n_clips]\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_midi_paths(cav_number, n_clips):\n",
    "        fmt_cav_paths = [os.path.join(SPLIT_PATH, i) for i in os.listdir(SPLIT_PATH) if i != f\"{cav_number}_cav\"]\n",
    "        all_cavs = []\n",
    "        for path in fmt_cav_paths:\n",
    "            all_cavs.extend([os.path.join(SPLIT_PATH, path, i) for i in os.listdir(path) if i.endswith('.mid')])\n",
    "        if n_clips is not None:\n",
    "            shuffle(all_cavs)\n",
    "            all_cavs = all_cavs[:n_clips]\n",
    "        return all_cavs\n",
    "\n",
    "\n",
    "vl = DataLoader(VoicingLoaderFake(15, n_clips=10), shuffle=True)\n",
    "len(vl.dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
